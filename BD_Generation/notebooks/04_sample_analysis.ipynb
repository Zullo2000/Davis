{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 — Sample Analysis\n",
    "\n",
    "Generate samples from a trained checkpoint, visualize them, and compute evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nfrom pathlib import Path\n\nimport torch\nimport matplotlib.pyplot as plt\n\n# Ensure BD_Generation is importable\nPROJECT_ROOT = Path.cwd().parent\nif str(PROJECT_ROOT) not in sys.path:\n    sys.path.insert(0, str(PROJECT_ROOT))\n\nfrom bd_gen.data.vocab import VocabConfig, RPLAN_VOCAB_CONFIG, NODE_PAD_IDX, NODE_TYPES, EDGE_TYPES\nfrom bd_gen.data.tokenizer import detokenize\nfrom bd_gen.data.dataset import BubbleDiagramDataset\nfrom bd_gen.model.denoiser import BDDenoiser\nfrom bd_gen.diffusion.noise_schedule import get_noise\nfrom bd_gen.diffusion.sampling import sample\nfrom bd_gen.utils.checkpoint import load_checkpoint\nfrom bd_gen.eval.validity import check_validity, check_validity_batch\nfrom bd_gen.eval.metrics import validity_rate, diversity, novelty, distribution_match, graph_structure_mmd\nfrom bd_gen.viz.graph_viz import draw_bubble_diagram, draw_bubble_diagram_grid\n\nvc = RPLAN_VOCAB_CONFIG\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Device: {device}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint: c:\\Users\\Alessandro Zuliani\\Documents\\Python_projects\\Davis\\BD_Generation\\checkpoint_final.pt\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint path (relative to PROJECT_ROOT)\n",
    "CHECKPOINT_PATH = PROJECT_ROOT / \"checkpoint_final.pt\"\n",
    "\n",
    "# Model config (must match training config)\n",
    "model = BDDenoiser(\n",
    "    d_model=128, n_layers=4, n_heads=4,\n",
    "    vocab_config=vc, dropout=0.0,\n",
    ").to(device)\n",
    "\n",
    "load_checkpoint(CHECKPOINT_PATH, model, device=device)\n",
    "model.eval()\n",
    "print(f\"Loaded checkpoint: {CHECKPOINT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "noise_cfg = OmegaConf.create({\"type\": \"linear\", \"sigma_min\": 0.0, \"sigma_max\": 10.0})\n",
    "noise_schedule = get_noise(noise_cfg).to(device)\n",
    "\n",
    "# Load training set for num_rooms distribution\n",
    "train_ds = BubbleDiagramDataset(\n",
    "    mat_path=PROJECT_ROOT / \"data\" / \"data.mat\",\n",
    "    cache_path=PROJECT_ROOT / \"data_cache\" / \"graph2plan_nmax8.pt\",\n",
    "    vocab_config=vc, split=\"train\", seed=42,\n",
    ")\n",
    "\n",
    "NUM_SAMPLES = 100\n",
    "\n",
    "with torch.no_grad():\n",
    "    tokens = sample(\n",
    "        model=model, noise_schedule=noise_schedule, vocab_config=vc,\n",
    "        batch_size=NUM_SAMPLES, num_steps=100, temperature=0.0,\n",
    "        num_rooms_distribution=train_ds.num_rooms_distribution,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "print(f\"Generated {tokens.shape[0]} samples, shape: {tokens.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct pad masks and detokenize\n",
    "graph_dicts = []\n",
    "pad_masks = torch.zeros_like(tokens, dtype=torch.bool)\n",
    "\n",
    "for i in range(tokens.size(0)):\n",
    "    node_toks = tokens[i, :vc.n_max]\n",
    "    nr = int((node_toks != NODE_PAD_IDX).sum().item())\n",
    "    nr = max(1, min(nr, vc.n_max))\n",
    "    pad_masks[i] = vc.compute_pad_mask(nr)\n",
    "    try:\n",
    "        gd = detokenize(tokens[i], pad_masks[i], vc)\n",
    "        graph_dicts.append(gd)\n",
    "    except ValueError:\n",
    "        graph_dicts.append({\"num_rooms\": 0, \"node_types\": [], \"edge_triples\": []})\n",
    "\n",
    "# Visualize first 16\n",
    "viz_dicts = [g for g in graph_dicts[:16] if g[\"num_rooms\"] > 0]\n",
    "fig = draw_bubble_diagram_grid(viz_dicts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Validity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = check_validity_batch(tokens, pad_masks, vc)\n",
    "\n",
    "v_rate = validity_rate(results)\n",
    "connected_rate = sum(1 for r in results if r[\"connected\"]) / len(results)\n",
    "valid_types_rate = sum(1 for r in results if r[\"valid_types\"]) / len(results)\n",
    "no_mask_rate = sum(1 for r in results if r[\"no_mask_tokens\"]) / len(results)\n",
    "\n",
    "print(f\"Validity rate:    {v_rate:.1%}\")\n",
    "print(f\"Connected rate:   {connected_rate:.1%}\")\n",
    "print(f\"Valid types rate: {valid_types_rate:.1%}\")\n",
    "print(f\"No MASK rate:     {no_mask_rate:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detokenize training set\n",
    "train_dicts = []\n",
    "for idx in range(min(len(train_ds), 5000)):  # Subsample for speed\n",
    "    item = train_ds[idx]\n",
    "    try:\n",
    "        gd = detokenize(item[\"tokens\"], item[\"pad_mask\"], vc)\n",
    "        train_dicts.append(gd)\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "div = diversity(graph_dicts)\n",
    "nov = novelty(graph_dicts, train_dicts)\n",
    "dm = distribution_match(graph_dicts, train_dicts)\n",
    "\n",
    "print(f\"Diversity: {div:.3f}\")\n",
    "print(f\"Novelty:   {nov:.3f}\")\n",
    "print(f\"KL divergence — nodes: {dm['node_kl']:.4f}, edges: {dm['edge_kl']:.4f}, rooms: {dm['num_rooms_kl']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Distribution Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Node type distributions\n",
    "sample_nodes = Counter(nt for g in graph_dicts for nt in g[\"node_types\"])\n",
    "train_nodes = Counter(nt for g in train_dicts for nt in g[\"node_types\"])\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Node types\n",
    "x = range(len(NODE_TYPES))\n",
    "s_total = max(sum(sample_nodes.values()), 1)\n",
    "t_total = max(sum(train_nodes.values()), 1)\n",
    "axes[0].bar([i - 0.2 for i in x], [sample_nodes.get(i, 0) / s_total for i in x], 0.4, label=\"Generated\")\n",
    "axes[0].bar([i + 0.2 for i in x], [train_nodes.get(i, 0) / t_total for i in x], 0.4, label=\"Training\")\n",
    "axes[0].set_xticks(list(x))\n",
    "axes[0].set_xticklabels([n[:4] for n in NODE_TYPES], rotation=45, ha=\"right\")\n",
    "axes[0].set_title(\"Node Type Distribution\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Edge types\n",
    "sample_edges = Counter(rel for g in graph_dicts for _, _, rel in g[\"edge_triples\"])\n",
    "train_edges = Counter(rel for g in train_dicts for _, _, rel in g[\"edge_triples\"])\n",
    "x = range(len(EDGE_TYPES))\n",
    "s_total = max(sum(sample_edges.values()), 1)\n",
    "t_total = max(sum(train_edges.values()), 1)\n",
    "axes[1].bar([i - 0.2 for i in x], [sample_edges.get(i, 0) / s_total for i in x], 0.4, label=\"Generated\")\n",
    "axes[1].bar([i + 0.2 for i in x], [train_edges.get(i, 0) / t_total for i in x], 0.4, label=\"Training\")\n",
    "axes[1].set_xticks(list(x))\n",
    "axes[1].set_xticklabels([e[:6] for e in EDGE_TYPES], rotation=45, ha=\"right\")\n",
    "axes[1].set_title(\"Edge Type Distribution\")\n",
    "axes[1].legend()\n",
    "\n",
    "# Num rooms\n",
    "sample_rooms = Counter(g[\"num_rooms\"] for g in graph_dicts)\n",
    "train_rooms = Counter(g[\"num_rooms\"] for g in train_dicts)\n",
    "x = range(1, vc.n_max + 1)\n",
    "s_total = max(sum(sample_rooms.values()), 1)\n",
    "t_total = max(sum(train_rooms.values()), 1)\n",
    "axes[2].bar([i - 0.2 for i in x], [sample_rooms.get(i, 0) / s_total for i in x], 0.4, label=\"Generated\")\n",
    "axes[2].bar([i + 0.2 for i in x], [train_rooms.get(i, 0) / t_total for i in x], 0.4, label=\"Training\")\n",
    "axes[2].set_xticks(list(x))\n",
    "axes[2].set_title(\"Num Rooms Distribution\")\n",
    "axes[2].legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Graph Structure MMD\n\nTopology-based metrics following DiGress / MELD / GraphARM evaluation protocol.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "mmd = graph_structure_mmd(graph_dicts, train_dicts, n_max=vc.n_max)\n\nprint(f\"MMD-Degree:     {mmd['mmd_degree']:.6f}\")\nprint(f\"MMD-Clustering: {mmd['mmd_clustering']:.6f}\")\nprint(f\"MMD-Spectral:   {mmd['mmd_spectral']:.6f}\")\nprint()\nprint(\"(Lower = more similar to training set. 0 = identical distribution.)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}